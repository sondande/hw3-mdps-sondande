MDP File:

Recall that every MDP has four parts: a set of states 𝑆, a set of actions 𝐴, a state transition function 𝑇(𝑠!, 𝑎, 𝑠!"#) = 𝑃(𝑠!"#|𝑠!, 𝑎), and a reward function 𝑅(𝑠!, 𝑎, 𝑠!"#). The MDP file format encodes these details as follows:

• Under the States heading is a list of all states in the environment, one state per line. Each state is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each state (ranging from 0-2303) and Label is a textual description of the state, here the values of each of the individual state variables (x, y, F0, F1, F2, F3) [described at the end of this document]. For purposes of this assignment, Label is only included to help you debug; otherwise, you will always refer to a state by its UniqueID.

• Under the Actions heading is a list of all actions that the agent can take in the environment, one action per line. Similar to states, each action is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each action (ranging from 0-4) and Label is a textual description of the action (“Extinguish”, “Up”, “Down”, “Left”, “Right”). As with states, the Label is only there to help you debug; otherwise, you will always refer to an action by its UniqueID.

• Under the State Transitions heading is a list of all state transitions that do not have 0% probability. Each state transition is recorded in the format:
state,action,next_state,probability

    where state represents the unique identifier of the current state 𝑠! ,
    action represents the unique identifier of the chosen action 𝑎,
    next_state represents the unique identifier of the next state 𝑠!"#,
    and probability represents the transition probability.

Of note: any state transition 𝑇(𝑠!, 𝑎, 𝑠!"#) not listed in the MDP file has 0% probability.

• Under the Rewards heading is a list of all reward combinations that do not have 0 value.
  Each reward is recorded in the format:

                    state,action,next_state,reward

    where state represents the unique identifier of the current state 𝑠! ,
    action represents the unique identifier of the chosen action 𝑎,
    next_state represents the unique identifier of the next state 𝑠!"#,
    and probability represents the transition probability.


    Of note: any reward 𝑅(𝑠!, 𝑎, 𝑠!"#) not listed in the MDP file has 0 value.

Data Structures:
    - State Dictionary from UniqueID to Label
    - Action Dictionary from UniqueID to Label
    - State Transitions Dictionary with 4 layers-- State, action, next_state, probability
    - Reward Transition Dictionary with 4 layers-- State, action, next_state, reward


Programming Assignment:

1. Parses a MDP file according to the above format into states, actions, state transitions, and rewards
2. Performs the Value Iteration algorithm to create the appropriate Q and V tables, then determines an appropriate policy from the resulting Q table.
   You should use 𝝐 = 𝟎. 𝟏 in Value Iteration, whereas 𝛾 will be specified by the user (see below).

Create state, action, state transitions, reward transitions dictionaries
Parse through the file:
    At state header:
        iterate through placing id's and labels into Dictionary
    At Actions header:
        iterate through placing id's and labels into Dictionary
    At State Transitions header:
        iterate through placing State, action, next_state, probability
    At Rewards:
        iterate through placing State, action, next_state, reward


2. Performs the Value Iteration algorithm to create the appropriate Q and V tables, then determines an appropriate policy from the resulting Q table. You should use 𝝐 = 𝟎. 𝟏 in Value Iteration, whereas 𝛾 will be specified by the user (see below).
3. Saves the policy to a file so that it can later be used in a simulation of the environment.

Your program should save the policy to a file in the following format. Each line should represent a single state and the prescribed action for that state.
The first value in a line is the state’s unique identifier, followed by a comma, followed by the action’s unique identifier. Since there are 2304 states in this problem, your policy file should have 2304 lines.

System Call:

python3 mdp_planner.py <mdpFilename> <gammaValue> <policyFilename>

    where <mdpFilename> is the path to the MDP file, <gammaValue> is a value to use for 𝛾 in the Value Iteration algorithm, and <mdpFilename> is the name of the file where your policy will be saved.

###################################################################################################################################################

Definition:
    - Stochastic:
        - actions can result in multiple possible next states, each with different probabilities
    - Non-episodic:
        - the environment might not have a fixed goal or end to a task
    - Rewards vs. costs:
        - agents might also receive positive rewards for taking certain actions in certain states

Development Plan:

    Stochastic Situation:
        ❖ Consider the Wildfire Suppression Problem
            ❖ Stochastic changes to the environment:
                ❖ New fires randomly appear (e.g., lightning strike)
                ❖ Wind changes speed/direction causes shift in fire movement
                ❖ Takeaway: cannot always accurately predict how environment will change over time
                    ❖ Including what will happen when a robot acts


        ❖ Let’s first look at how to model stochastic environments
            ❖ What are probabilities?
            ❖ What influences the probabilities ?
                1. Environment changes randomly, independent of the agent
                2 .Agent’s actions influence the environment changes


    Notes to look up: going into how we can implement the probability theory to be more accurate
        Implement Conditional probabilities??
            Definition:
                - Conditional probabilities:
                    - P(B | A) = probability of event B given that event A already happened

                    Examples:
                        ❖ Vaccines: P(infected | vaccinated) = 0.01
                        ❖ Sports: P(win | score  50) = 0.9

                    #### Note on Conditional Probability: ####
                    ❖ Given some event A, sum of all other events is still equal to 1
                    ❖ P(win | score  50) + P(lose | score  50) = 1
                        “After A, something else has to also happen”
                            ❖ ∑ P(B|A) = 1

        Law of total probability: P(B)=∑AP(B|A)P(A)
            ❖ P(win) = P(win | score  50) * P(score  50) + P(win | score < 50) * P(score < 50)
            ❖ The probability of a later event depends on all the things that could have happened earlier

        ❖ P(A, B) = joint probability of event A andB
            ❖ If events are independent: P(A, B) = P(A) * P(B)
                ❖ 2 Coin Flips: P(heads, tails) = P(heads) * P(tails) = 0.5 * 0.5 = 0.25
                ❖ P(B | A) = P(B) [A doesn’t change the probability of B]
            ❖ If events are dependent:   P(A, B) = P(B|A) * P(A)
                ❖ Marbles: P(red, blue) = P(blue | red) * P(red) = 5/49 * 10/50 = 0.0204

    Markov Chain (in our code will be represented as a Dictionary as the Data Structure)

        ❖ Model of stochastic environment: Markov Chain <S, T>
            ❖ States S = {s}
            ❖ State Transition Function T(st, st+1) =  P(st+1|st)
                ❖ Informs us how likely one state is to follow another
                ❖ Assume that we already know this (a priori)

        ❖ Markov property: the probability of the future depends only on the present and not the past
            ❖ Current state st is a sufficient statistic for summarizing all of history❖P(st+1|st) = P(st+1|st, st-1, st-2, ..., s0)
            ❖ In the real-world, this assumption doesn’t always hold, but it still gives us interesting results

        ❖ Using Markov Chains, we can answer questions like:
            ❖ What is the likely next state to occur?
            ❖ What is the probability of a sequence of states?
            ❖ Given two or more chains, which one most likely produced a given sequence of states?

    Markov Decision Process
    ❖ When the agent influences state transitions, we have aMarkov Decision Process   MDP = <S, A, T, R>
        ❖ States of the environment S = {s}
        ❖ Actions the agent can choose A = {a}
        ❖ Stochastic state transition function
            ❖ T(st, a, st+1) = P(st+1| st, a) = probability state goes from st to st+1 after agent takes action a
            ❖ Reward function
                ❖ R(st, a, st+1) = reward for taking action a in state stand reaching state st+1
                ❖ Often the next state doesn’t change the reward, so we have R(st, a)

    Wildfire Suppression Model
        ❖ States: factored combination of fire and suppressant levels
            ❖ Fire Variables: model environment as a grid/graph of locations, each with a different fire level
                ❖ 0 = no fire
                ❖ 1, 2, ..., k-1 = different levels of fire intensity
                ❖ k = burned out
            ❖ Suppressant Variable: how much suppressant resource does the agent have?
                ❖ Fighting fires expends suppressant, and the agent needs to recharge when it runs out

        ❖ Actions: what can the agent do to put out fires?
            ❖ If the agent has suppressant, choose a nearby fire to fight
                ❖ Can pick from any location within range of the agent
            ❖ If the agent has no suppressant: leave the fires and recharge
            ❖ If there are no fires: wait until needed

        ❖ State Transitions: stochastic changes to fires and suppressant
            ❖ Fire Variables: fire intensity levels naturally increase over time (based on Boychuk et al., 2009)
                ❖ If not on fire, probability of catching fire depends increases with number of neighboring locations on fire (factoring in wind, vegetation, etc.)
                ❖ If on fire, intensity either stays the same or increases one level
                ❖ Unless an agent fights a fire, then the intensity decreases
            ❖ Suppressant Variable: suppressant decreases with each use or increases with recharging

        ❖ Rewards: signal to the agent about the quality of actions in different states
            ❖ Our model: +1 for each location not on fire
                ❖ Encourages the agent to put out fires as fast as possible
        ❖ Agent’sGoal: choose actions that give the greatest cumulative reward over time
            ❖ Protect from fires spreading and burning all the vegetation in neighboring areas

    #### Read to define policy that we will use in the program ####
    ❖ Solving a MDP creates a plan of actions for the agent
        ❖ Called a policy π: S→A
            ❖ Determines which action a = π(s) that the agent should take in a given state s
        ❖ Can also have a stochastic policy π: S × A→ [0,1]
            ❖ Determines the likelihood P(a | s) = π(s, a) that the agent should choose action a in a given state s
    ❖ Optimal policy: one that maximizes the utility of the agent
        ❖ Wildfire suppression: tells the agent which fires to fight and when to recharge its suppressant
        ❖ Game show: tells the agent whether to keep playing or quit

    From Rewards to Utilities
        ❖ Want utility function U(s) to balance immediate and long-term benefits to the agent
            ❖ Cumulative rewards:
                - Uπ(s0)=R(s0,π(s0),s1)+R(s1,π(s1),s2)+R(s2,π(s2),s3)+...=∞∑t=0 R(st,π(st),st+1)

            ❖ Discounted cumulative rewards:
                - Uπ(s0)=R(s0,π(s0),s1)+γR(s1,π(s1),s2)+γ2R(s2,π(s2),s3)+...=∞∑t=0 γ^t R(st,π(st),st+1)
              where γ is a discount factor in the range (0, 1)

              Reference slides for the information about the equations

    Bellman Equation
        ❖ At the core of planning is calculating expected utility V(s) for each state s from the reward function R
        Q(st,a)=∑st+1∈ST(st,a,st+1)[R(st,a,st+1)+γ*V(st+1)]

        #### Reference slides for the information about the equations ####

        ❖ Do this for every state s and action a (repeatedly until converge)

        Reference the next slides for the situation where the reward does not depend on next state:


Planning with MDPs
    ❖ Many different algorithms for calculating π(s).  Two general approaches:
        ❖ Offline planning: calculate π(s) for all possible states s in advance before the agent executes the plan in the environment
        ❖ Online planning: calculate π(s) for current state s, then execute, then plan again, execute new plan, repeat.


Applying Offline Planning:
# To allow us to evaluate the information before we execute it.
# Prepare all the information, confirm it is correct (if possible), the execute it.
    # - Good for debugging as check in points

    Planning (Calculate Utilities, Pick Best Action):
        - Calculate Utilities
            - Function purpose:
            - Parameters to take in:
            - Where is this function used:
            - What do we need to calculate this in a:
                - Stochastic environment:
                    -
                - Non-episodic:
                    -
            - What do we want to return?
                - What type is our result?
                    - Object?
                    - Variable (string, int, boolean, etc.)

        - Pick Best Action
            - Function purpose:
            - Parameters to take in:
            - Where is this function used:
            - What do we need to calculate this in a:
                - Stochastic environment:
                    -
                - Non-episodic:
                    -
            - What do we want to return?
                - What type is our result?
                    - Object?
                    - Variable (string, int, boolean, etc.)

    Execute Plan:
        - How can we do this?
            -
            ####
            - Function purpose:
                - Parameters to take in:
                - Where is this function used:
                - What do we need to calculate this in a:
                    - Stochastic environment:
                        -
                    - Non-episodic:
                        -
                - What do we want to return?
                    - What type is our result?
                        - Object?
                        - Variable (string, int, boolean, etc.)
                        
            ####
            - In the case of recursion:
                - What result are we looking for?
                    - What is our base case?
                        -
                    - What is our recursive case?
                        -

From HW3 helping slide:

    Markov Decision Process:
        ❖ Markov Decision Process   MDP = <S, A, T, R>
        ❖ States of the environment S = {s}
        ❖ Actions the agent can choose A = {a}
        ❖ Stochastic state transition function
            ❖ T(st, a, st+1) = P(st+1| st, a) = probability state goes from st to st+1 after agent takes action a
        ❖ Reward function
            ❖ R(st, a, st+1) = reward for taking action a in state stand reaching state st+1

    Using a List over a set for the data structure:
        Iterate through each state because it has a unique number 0-2303
        For loops will iterate over states in order of their unique number
            - Results in the value iteration algorithm being slightly faster

    Functions as Maps:
        ❖ What is a function?
            ❖ Map of inputs to outputs
        ❖ What is a dictionary or HashMap?
            ❖ Map of keys to values
        ❖ Every function can be represented as a Dictionary/HashMap
            ❖ Keys = inputs
            ❖ Values = outputs

    State Transition Map:
        ❖ Inputs for the state transition function T:
            ❖ Current state st, chosen action a, next state st+1
            ❖ Map of three inputs to a single output number
        ❖ Two possible representations
            ❖ transitions[state][action][next_state] = probability
            ❖ transitions[(state, action, next_state)] = probability
            ❖ Only the full combinations of inputs can save the correct output (given in the MDP file)

Pseudocode for Value Iteration:
    1. Initialize Q table and V values to all zeros

    (Aka step 4. Repeat Steps 2-3 until V values converge * Largest change in V(s) is less than e)
    While loop (until the largest change in V < e aka the covergence criteria is met)
        2. Update entire Q table using Bellman equation
            For each state s in S:
                For each action a in A:
                    Calculate Q(s, a) with Bellman

        3. Update all V values using Q table
            For each state s in S:
                V(s) = max Q(s, a)

Question:
    - Ask about the reward equations
    - Do we want to use Lists or Dictionaries for out State transition? And if so, how would we make sure there are
      unique identifiers for each of the states, so we can use lists.