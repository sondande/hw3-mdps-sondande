MDP File:

Recall that every MDP has four parts: a set of states ğ‘†, a set of actions ğ´, a state transition function ğ‘‡(ğ‘ !, ğ‘, ğ‘ !"#) = ğ‘ƒ(ğ‘ !"#|ğ‘ !, ğ‘), and a reward function ğ‘…(ğ‘ !, ğ‘, ğ‘ !"#). The MDP file format encodes these details as follows:

â€¢ Under the States heading is a list of all states in the environment, one state per line. Each state is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each state (ranging from 0-2303) and Label is a textual description of the state, here the values of each of the individual state variables (x, y, F0, F1, F2, F3) [described at the end of this document]. For purposes of this assignment, Label is only included to help you debug; otherwise, you will always refer to a state by its UniqueID.

â€¢ Under the Actions heading is a list of all actions that the agent can take in the environment, one action per line. Similar to states, each action is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each action (ranging from 0-4) and Label is a textual description of the action (â€œExtinguishâ€, â€œUpâ€, â€œDownâ€, â€œLeftâ€, â€œRightâ€). As with states, the Label is only there to help you debug; otherwise, you will always refer to an action by its UniqueID.

â€¢ Under the State Transitions heading is a list of all state transitions that do not have 0% probability. Each state transition is recorded in the format:
state,action,next_state,probability

    where state represents the unique identifier of the current state ğ‘ ! ,
    action represents the unique identifier of the chosen action ğ‘,
    next_state represents the unique identifier of the next state ğ‘ !"#,
    and probability represents the transition probability.

Of note: any state transition ğ‘‡(ğ‘ !, ğ‘, ğ‘ !"#) not listed in the MDP file has 0% probability.

â€¢ Under the Rewards heading is a list of all reward combinations that do not have 0 value.
  Each reward is recorded in the format:

                    state,action,next_state,reward

    where state represents the unique identifier of the current state ğ‘ ! ,
    action represents the unique identifier of the chosen action ğ‘,
    next_state represents the unique identifier of the next state ğ‘ !"#,
    and probability represents the transition probability.


    Of note: any reward ğ‘…(ğ‘ !, ğ‘, ğ‘ !"#) not listed in the MDP file has 0 value.


Programming Assignment:

1. Parses a MDP file according to the above format into states, actions, state transitions, and rewards
2. Performs the Value Iteration algorithm to create the appropriate Q and V tables, then determines an appropriate policy from the resulting Q table.
   You should use ğ = ğŸ. ğŸ in Value Iteration, whereas ğ›¾ will be specified by the user (see below).
3. Saves the policy to a file so that it can later be used in a simulation of the environment.

Your program should save the policy to a file in the following format. Each line should represent a single state and the prescribed action for that state.
The first value in a line is the stateâ€™s unique identifier, followed by a comma, followed by the actionâ€™s unique identifier. Since there are 2304 states in this problem, your policy file should have 2304 lines.

System Call:

python3 mdp_planner.py <mdpFilename> <gammaValue> <policyFilename>

    where <mdpFilename> is the path to the MDP file, <gammaValue> is a value to use for ğ›¾ in the Value Iteration algorithm, and <mdpFilename> is the name of the file where your policy will be saved.

###################################################################################################################################################

Definition:
    - Stochastic:
        - actions can result in multiple possible next states, each with different probabilities
    - Non-episodic:
        - the environment might not have a fixed goal or end to a task
    - Rewards vs. costs:
        - agents might also receive positive rewards for taking certain actions in certain states

Development Plan:

    Stochastic Situation:
        â– Consider the Wildfire Suppression Problem
            â– Stochastic changes to the environment:
                â– New fires randomly appear (e.g., lightning strike)
                â– Wind changes speed/direction causes shift in fire movement
                â– Takeaway: cannot always accurately predict how environment will change over time
                    â– Including what will happen when a robot acts


        â– Letâ€™s first look at how to model stochastic environments
            â– What are probabilities?
            â– What influences the probabilities ?
                1. Environment changes randomly, independent of the agent
                2 .Agentâ€™s actions influence the environment changes


    Notes to look up: going into how we can implement the probability theory to be more accurate
        Implement Conditional probabilities??
            Definition:
                - Conditional probabilities:
                    - P(B | A) = probability of event B given that event A already happened

                    Examples:
                        â– Vaccines: P(infected | vaccinated) = 0.01
                        â– Sports: P(win | score  50) = 0.9

                    #### Note on Conditional Probability: ####
                    â– Given some event A, sum of all other events is still equal to 1
                    â– P(win | score  50) + P(lose | score  50) = 1
                        â€œAfter A, something else has to also happenâ€
                            â– âˆ‘ P(B|A) = 1

        Law of total probability: P(B)=âˆ‘AP(B|A)P(A)
            â– P(win) = P(win | score  50) * P(score  50) + P(win | score < 50) * P(score < 50)
            â– The probability of a later event depends on all the things that could have happened earlier

        â– P(A, B) = joint probability of event A andB
            â– If events are independent: P(A, B) = P(A) * P(B)
                â– 2 Coin Flips: P(heads, tails) = P(heads) * P(tails) = 0.5 * 0.5 = 0.25
                â– P(B | A) = P(B) [A doesnâ€™t change the probability of B]
            â– If events are dependent:   P(A, B) = P(B|A) * P(A)
                â– Marbles: P(red, blue) = P(blue | red) * P(red) = 5/49 * 10/50 = 0.0204

    Markov Chain (in our code will be represented as a Dictionary as the Data Structure)

        â– Model of stochastic environment: Markov Chain <S, T>
            â– States S = {s}
            â– State Transition Function T(st, st+1) =  P(st+1|st)
                â– Informs us how likely one state is to follow another
                â– Assume that we already know this (a priori)

        â– Markov property: the probability of the future depends only on the present and not the past
            â– Current state st is a sufficient statistic for summarizing all of historyâ–P(st+1|st) = P(st+1|st, st-1, st-2, ..., s0)
            â– In the real-world, this assumption doesnâ€™t always hold, but it still gives us interesting results

        â– Using Markov Chains, we can answer questions like:
            â– What is the likely next state to occur?
            â– What is the probability of a sequence of states?
            â– Given two or more chains, which one most likely produced a given sequence of states?