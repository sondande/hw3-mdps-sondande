MDP File:

Recall that every MDP has four parts: a set of states ğ‘†, a set of actions ğ´, a state transition function ğ‘‡(ğ‘ !, ğ‘, ğ‘ !"#) = ğ‘ƒ(ğ‘ !"#|ğ‘ !, ğ‘), and a reward function ğ‘…(ğ‘ !, ğ‘, ğ‘ !"#). The MDP file format encodes these details as follows:

â€¢ Under the States heading is a list of all states in the environment, one state per line. Each state is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each state (ranging from 0-2303) and Label is a textual description of the state, here the values of each of the individual state variables (x, y, F0, F1, F2, F3) [described at the end of this document]. For purposes of this assignment, Label is only included to help you debug; otherwise, you will always refer to a state by its UniqueID.

â€¢ Under the Actions heading is a list of all actions that the agent can take in the environment, one action per line. Similar to states, each action is recorded in the format:
                               UniqueID,Label
    where UniqueID is a unique number identifier for each action (ranging from 0-4) and Label is a textual description of the action (â€œExtinguishâ€, â€œUpâ€, â€œDownâ€, â€œLeftâ€, â€œRightâ€). As with states, the Label is only there to help you debug; otherwise, you will always refer to an action by its UniqueID.

â€¢ Under the State Transitions heading is a list of all state transitions that do not have 0% probability. Each state transition is recorded in the format:
state,action,next_state,probability

    where state represents the unique identifier of the current state ğ‘ ! , action represents the unique identifier of the chosen action ğ‘, next_state represents the unique identifier of the next state ğ‘ !"#, and probability represents the transition probability.

Of note: any state transition ğ‘‡(ğ‘ !, ğ‘, ğ‘ !"#) not listed in the MDP file has 0% probability.

â€¢ Under the Rewards heading is a list of all reward combinations that do not have 0 value.
  Each reward is recorded in the format:

                    state,action,next_state,reward

    where state represents the unique identifier of the current state ğ‘ ! , action represents the unique identifier of the chosen action ğ‘, next_state represents the unique identifier of the next state ğ‘ !"#, and probability represents the transition probability.

    Of note: any reward ğ‘…(ğ‘ !, ğ‘, ğ‘ !"#) not listed in the MDP file has 0 value.

Data Structures:
    - State Dictionary from UniqueID to Label
    - Action Dictionary from UniqueID to Label
    - State Transitions Dictionary with 4 layers-- State, action, next_state, probability
    - Reward Transition Dictionary with 4 layers-- State, action, next_state, reward

Programming Assignment:

1. Parses a MDP file according to the above format into states, actions, state transitions, and rewards

Create state, action, state transitions, reward transitions dictionaries
Parse through the file:
    At state header:
        iterate through placing id's and labels into Dictionary
    At Actions header:
        iterate through placing id's and labels into Dictionary
    At State Transitions header: 
        iterate through placing State, action, next_state, probability
    At Rewards:
        iterate through placing State, action, next_state, reward


2. Performs the Value Iteration algorithm to create the appropriate Q and V tables, then determines an appropriate policy from the resulting Q table. You should use ğ = ğŸ. ğŸ in Value Iteration, whereas ğ›¾ will be specified by the user (see below).
3. Saves the policy to a file so that it can later be used in a simulation of the environment.

Your program should save the policy to a file in the following format. Each line should represent a single state and the prescribed action for that state. The first value in a line is the stateâ€™s unique identifier, followed by a comma, followed by the actionâ€™s unique identifier. Since there are 2304 states in this problem, your policy file should have 2304 lines.

System Call:

python3 mdp_planner.py <mdpFilename> <gammaValue> <policyFilename>

    where <mdpFilename> is the path to the MDP file, <gammaValue> is a value to use for ğ›¾ in the Value Iteration algorithm, and <mdpFilename> is the name of the file where your policy will be saved.